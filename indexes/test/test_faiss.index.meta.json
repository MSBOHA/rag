[{"source": "test.md", "text": "# 哆啦A梦与超级赛亚人：时空之战\n\n在一个寻常的午后，大雄依旧坐在书桌前发呆，作业堆得像山，连第一"}, {"source": "test.md", "text": "作业堆得像山，连第一页都没动。哆啦A梦在一旁翻着漫画，时不时叹口气，觉得这孩子还是一如既往的不靠谱。"}, {"source": "test.md", "text": "是一如既往的不靠谱。正当他们的生活照常进行时，一道强光突然从天而降，整个房间震动不已。光芒中走出一名"}, {"source": "test.md", "text": "不已。光芒中走出一名金发少年，身披战甲、气势惊人，他就是来自未来的超级赛亚人——特兰克斯。他一出现便"}, {"source": "test.md", "text": "特兰克斯。他一出现便说出了惊人的话：未来的地球即将被黑暗势力摧毁，他来此是为了寻求哆啦A梦的帮助。"}, {"source": "test.md", "text": "求哆啦A梦的帮助。\n\n哆啦A梦与大雄听后大惊，但也从特兰克斯坚定的眼神中读出了不容拒绝的决心。特兰克"}, {"source": "test.md", "text": "容拒绝的决心。特兰克斯解释说，未来的敌人并非普通反派，而是一个名叫“黑暗赛亚人”的存在，他由邪恶科学"}, {"source": "test.md", "text": "的存在，他由邪恶科学家复制了贝吉塔的基因并加以改造，实力超乎想象。这个敌人不仅拥有赛亚人战斗力，还能"}, {"source": "test.md", "text": "有赛亚人战斗力，还能操纵扭曲的时间能量，几乎无人可敌。特兰克斯已经独自战斗多年，但每一次都以惨败告终"}, {"source": "test.md", "text": "但每一次都以惨败告终。他说：“科技，是我那个时代唯一缺失的武器，而你们，正好拥有它。”\n\n于是，哆啦"}, {"source": "test.md", "text": "它。”\n\n于是，哆啦A梦带着特兰克斯与大雄启动时光机，穿越到了那个即将崩溃的未来世界。眼前的景象令人"}, {"source": "test.md", "text": "世界。眼前的景象令人震撼：城市沦为废墟，大地裂痕纵横，天空中浮动着压抑的黑雾。特兰克斯说，这正是黑暗"}, {"source": "test.md", "text": "兰克斯说，这正是黑暗赛亚人带来的结果，一切生命几乎都被抹杀，只剩他在苦苦支撑。大雄虽感到恐惧，但看到"}, {"source": "test.md", "text": "雄虽感到恐惧，但看到无辜的人类遭殃，内心逐渐燃起斗志。哆啦A梦则冷静地分析局势，决定使用他最强的三样"}, {"source": "test.md", "text": "决定使用他最强的三样秘密道具来对抗黑暗势力。\n\n三件秘密道具分别是：可以临时赋予超级战力的“复制斗篷"}, {"source": "test.md", "text": "超级战力的“复制斗篷”，能暂停时间五秒的“时间停止手表”，以及可在一分钟中完成一年修行的“精神与时光"}, {"source": "test.md", "text": "年修行的“精神与时光屋便携版”。大雄被推进精神屋内，在其中接受密集的训练，虽然只有几分钟现实时间，他"}, {"source": "test.md", "text": "有几分钟现实时间，他却经历了整整一年的苦修。刚开始他依旧软弱，想放弃、想逃跑，但当他想起静香、父母，"}, {"source": "test.md", "text": "当他想起静香、父母，还有哆啦A梦那坚定的眼神时，他终于咬牙坚持了下来。出来之后，他的身体与精神都焕然"}, {"source": "test.md", "text": "他的身体与精神都焕然一新，眼神中多了一份成熟与自信。\n\n最终战在黑暗赛亚人的空中要塞前爆发，特兰克斯"}, {"source": "test.md", "text": "要塞前爆发，特兰克斯率先出击，释放全力与敌人正面对决。哆啦A梦则用任意门和道具支援，从各个方向制造混"}, {"source": "test.md", "text": "援，从各个方向制造混乱，尽量压制敌人的时空能力。但黑暗赛亚人太过强大，仅凭特兰克斯一人根本无法压制，"}, {"source": "test.md", "text": "斯一人根本无法压制，更别说击败。就在特兰克斯即将被击倒之际，大雄披上复制斗篷、冲破恐惧从高空跃下。他"}, {"source": "test.md", "text": "破恐惧从高空跃下。他的拳头燃烧着金色光焰，目标直指敌人心脏。\n\n时间停止装置在关键时刻启动，世界陷入"}, {"source": "test.md", "text": "键时刻启动，世界陷入静止，大雄用这个短短五秒接近了敌人的盲点。他集中全力，一记重拳击穿了黑暗赛亚人的"}, {"source": "test.md", "text": "拳击穿了黑暗赛亚人的能量核心，引发巨大的能量反冲。黑暗赛亚人尖叫着化为碎光，天空中的黑雾瞬间散去，阳"}, {"source": "test.md", "text": "中的黑雾瞬间散去，阳光重新洒落大地。特兰克斯倒在地上，看着眼前这个曾经懦弱的少年，露出了欣慰的笑容。"}, {"source": "test.md", "text": "，露出了欣慰的笑容。他知道，这一次，是大雄救了世界。\n\n战后，未来世界开始恢复，植物重新生长，人类重"}, {"source": "test.md", "text": "植物重新生长，人类重建家园。特兰克斯告别时紧紧握住大雄的手，说：“你是我见过最特别的战士。”哆啦A梦"}, {"source": "test.md", "text": "别的战士。”哆啦A梦也为大雄感到骄傲，说他终于真正成长了一次。三人站在山丘上，看着远方重新明亮的地平"}, {"source": "test.md", "text": "着远方重新明亮的地平线，心中感受到从未有过的安宁。随后，哆啦A梦与大雄乘坐时光机返回了属于他们的那个"}, {"source": "test.md", "text": "返回了属于他们的那个年代，一切仿佛又恢复平静。\n\n回到现代后，大雄仿佛变了一个人，不再轻易抱怨、不再"}, {"source": "test.md", "text": "，不再轻易抱怨、不再逃避责任。他认真写完作业，帮妈妈买菜，甚至主动练习体育，哆啦A梦惊讶得说不出话来"}, {"source": "test.md", "text": "A梦惊讶得说不出话来。他知道，这不是一时兴起，而是大雄真正内心成长的结果。大雄有时会望着天空出神，仿"}, {"source": "test.md", "text": "时会望着天空出神，仿佛还能看见未来世界的那一片废墟与重生的希望。他不会说出来，但他心中永远铭记那一战"}, {"source": "test.md", "text": "他心中永远铭记那一战。\n\n几天后，电视新闻中突然出现一则画面：一位金发少年在街头击退了失控的机器人，"}, {"source": "test.md", "text": "击退了失控的机器人，引发市民围观与猜测。大雄放下手中的课本，望向哆啦A梦，两人心照不宣地笑了。也许，"}, {"source": "test.md", "text": "照不宣地笑了。也许，特兰克斯又回来了，也许，新的敌人正在逼近。冒险从未真正结束，而他们，早已准备好了"}, {"source": "test.md", "text": "而他们，早已准备好了。无论时空如何动荡，他们将永远并肩作战。"}, {"source": "test.pdf", "text": "Provided proper attribution is provided, Google he"}, {"source": "test.pdf", "text": "Google hereby grants permission to\nreproduce the"}, {"source": "test.pdf", "text": "oduce the tables and figures in this paper solely"}, {"source": "test.pdf", "text": "er solely for use in journalistic or\nscholarly wor"}, {"source": "test.pdf", "text": "olarly works.\nAttention Is All You Need\nAshish Vas"}, {"source": "test.pdf", "text": "Ashish Vaswani∗\nGoogle Brain\navaswani@google.com\nN"}, {"source": "test.pdf", "text": "ogle.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.co"}, {"source": "test.pdf", "text": "@google.com\nNiki Parmar∗\nGoogle Research\nnikip@goo"}, {"source": "test.pdf", "text": "nikip@google.com\nJakob Uszkoreit∗\nGoogle Research"}, {"source": "test.pdf", "text": "e Research\nusz@google.com\nLlion Jones∗\nGoogle Rese"}, {"source": "test.pdf", "text": "oogle Research\nllion@google.com\nAidan N. Gomez∗†\nU"}, {"source": "test.pdf", "text": "Gomez∗†\nUniversity of Toronto\naidan@cs.toronto.ed"}, {"source": "test.pdf", "text": "toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkais"}, {"source": "test.pdf", "text": "lukaszkaiser@google.com\nIllia Polosukhin∗‡\nillia.p"}, {"source": "test.pdf", "text": "∗‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominan"}, {"source": "test.pdf", "text": "he dominant sequence transduction models are based"}, {"source": "test.pdf", "text": "are based on complex recurrent or\nconvolutional n"}, {"source": "test.pdf", "text": "lutional neural networks that include an encoder a"}, {"source": "test.pdf", "text": "encoder and a decoder. The best\nperforming models"}, {"source": "test.pdf", "text": "ing models also connect the encoder and decoder th"}, {"source": "test.pdf", "text": "decoder through an attention\nmechanism. We propose"}, {"source": "test.pdf", "text": "We propose a new simple network architecture, the"}, {"source": "test.pdf", "text": "ture, the Transformer,\nbased solely on attention m"}, {"source": "test.pdf", "text": "ttention mechanisms, dispensing with recurrence an"}, {"source": "test.pdf", "text": "urrence and convolutions\nentirely. Experiments on"}, {"source": "test.pdf", "text": "iments on two machine translation tasks show these"}, {"source": "test.pdf", "text": "show these models to\nbe superior in quality while"}, {"source": "test.pdf", "text": "ity while being more parallelizable and requiring"}, {"source": "test.pdf", "text": "requiring significantly\nless time to train. Our mo"}, {"source": "test.pdf", "text": "in. Our model achieves 28.4 BLEU on the WMT 2014 E"}, {"source": "test.pdf", "text": "WMT 2014 English-\nto-German translation task, impr"}, {"source": "test.pdf", "text": "task, improving over the existing best results, in"}, {"source": "test.pdf", "text": "esults, including\nensembles, by over 2 BLEU. On th"}, {"source": "test.pdf", "text": "LEU. On the WMT 2014 English-to-French translation"}, {"source": "test.pdf", "text": "ranslation task,\nour model establishes a new singl"}, {"source": "test.pdf", "text": "new single-model state-of-the-art BLEU score of 4"}, {"source": "test.pdf", "text": "score of 41.8 after\ntraining for 3.5 days on eight"}, {"source": "test.pdf", "text": "s on eight GPUs, a small fraction of the training"}, {"source": "test.pdf", "text": "training costs of the\nbest models from the litera"}, {"source": "test.pdf", "text": "the literature. We show that the Transformer gener"}, {"source": "test.pdf", "text": "rmer generalizes well to\nother tasks by applying i"}, {"source": "test.pdf", "text": "applying it successfully to English constituency p"}, {"source": "test.pdf", "text": "tituency parsing both with\nlarge and limited train"}, {"source": "test.pdf", "text": "ited training data.\n∗Equal contribution. Listing o"}, {"source": "test.pdf", "text": "Listing order is random. Jakob proposed replacing"}, {"source": "test.pdf", "text": "replacing RNNs with self-attention and started\nth"}, {"source": "test.pdf", "text": "started\nthe effort to evaluate this idea. Ashish,"}, {"source": "test.pdf", "text": ". Ashish, with Illia, designed and implemented the"}, {"source": "test.pdf", "text": "mented the first Transformer models and\nhas been c"}, {"source": "test.pdf", "text": "has been crucially involved in every aspect of thi"}, {"source": "test.pdf", "text": "ect of this work. Noam proposed scaled dot-product"}, {"source": "test.pdf", "text": "ot-product attention, multi-head\nattention and the"}, {"source": "test.pdf", "text": "on and the parameter-free position representation"}, {"source": "test.pdf", "text": "sentation and became the other person involved in"}, {"source": "test.pdf", "text": "volved in nearly every\ndetail. Niki designed, impl"}, {"source": "test.pdf", "text": "gned, implemented, tuned and evaluated countless m"}, {"source": "test.pdf", "text": "ountless model variants in our original codebase a"}, {"source": "test.pdf", "text": "codebase and\ntensor2tensor. Llion also experimente"}, {"source": "test.pdf", "text": "xperimented with novel model variants, was respons"}, {"source": "test.pdf", "text": "as responsible for our initial codebase, and\neffic"}, {"source": "test.pdf", "text": "and\nefficient inference and visualizations. Lukas"}, {"source": "test.pdf", "text": "ons. Lukasz and Aidan spent countless long days de"}, {"source": "test.pdf", "text": "ng days designing various parts of and\nimplementin"}, {"source": "test.pdf", "text": "mplementing tensor2tensor, replacing our earlier c"}, {"source": "test.pdf", "text": "earlier codebase, greatly improving results and m"}, {"source": "test.pdf", "text": "ults and massively accelerating\nour research.\n†Wor"}, {"source": "test.pdf", "text": "arch.\n†Work performed while at Google Brain.\n‡Work"}, {"source": "test.pdf", "text": "ain.\n‡Work performed while at Google Research.\n31s"}, {"source": "test.pdf", "text": "earch.\n31st Conference on Neural Information Proce"}, {"source": "test.pdf", "text": "tion Processing Systems (NIPS 2017), Long Beach, C"}, {"source": "test.pdf", "text": "g Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 A"}, {"source": "test.pdf", "text": "s.CL]  2 Aug 2023\n1\nIntroduction\nRecurrent neural"}, {"source": "test.pdf", "text": "nt neural networks, long short-term memory [13] an"}, {"source": "test.pdf", "text": "ry [13] and gated recurrent [7] neural networks\nin"}, {"source": "test.pdf", "text": "etworks\nin particular, have been firmly establishe"}, {"source": "test.pdf", "text": "established as state of the art approaches in sequ"}, {"source": "test.pdf", "text": "es in sequence modeling and\ntransduction problems"}, {"source": "test.pdf", "text": "problems such as language modeling and machine tr"}, {"source": "test.pdf", "text": "machine translation [35, 2, 5]. Numerous\nefforts h"}, {"source": "test.pdf", "text": "efforts have since continued to push the boundari"}, {"source": "test.pdf", "text": "e boundaries of recurrent language models and enco"}, {"source": "test.pdf", "text": "s and encoder-decoder\narchitectures [38, 24, 15]."}, {"source": "test.pdf", "text": "24, 15].\nRecurrent models typically factor comput"}, {"source": "test.pdf", "text": "tor computation along the symbol positions of the"}, {"source": "test.pdf", "text": "ns of the input and output\nsequences. Aligning the"}, {"source": "test.pdf", "text": "igning the positions to steps in computation time,"}, {"source": "test.pdf", "text": "tion time, they generate a sequence of hidden\nstat"}, {"source": "test.pdf", "text": "idden\nstates ht, as a function of the previous hid"}, {"source": "test.pdf", "text": "evious hidden state ht−1 and the input for positio"}, {"source": "test.pdf", "text": "or position t. This inherently\nsequential nature p"}, {"source": "test.pdf", "text": "l nature precludes parallelization within training"}, {"source": "test.pdf", "text": "n training examples, which becomes critical at lon"}, {"source": "test.pdf", "text": "cal at longer\nsequence lengths, as memory constrai"}, {"source": "test.pdf", "text": "y constraints limit batching across examples. Rece"}, {"source": "test.pdf", "text": "ples. Recent work has achieved\nsignificant improve"}, {"source": "test.pdf", "text": "nt improvements in computational efficiency throug"}, {"source": "test.pdf", "text": "ncy through factorization tricks [21] and conditio"}, {"source": "test.pdf", "text": "d conditional\ncomputation [32], while also improvi"}, {"source": "test.pdf", "text": "so improving model performance in case of the latt"}, {"source": "test.pdf", "text": "f the latter. The fundamental\nconstraint of sequen"}, {"source": "test.pdf", "text": "of sequential computation, however, remains.\nAtte"}, {"source": "test.pdf", "text": "ains.\nAttention mechanisms have become an integral"}, {"source": "test.pdf", "text": "n integral part of compelling sequence modeling an"}, {"source": "test.pdf", "text": "odeling and transduc-\ntion models in various tasks"}, {"source": "test.pdf", "text": "ious tasks, allowing modeling of dependencies with"}, {"source": "test.pdf", "text": "ncies without regard to their distance in\nthe inpu"}, {"source": "test.pdf", "text": "n\nthe input or output sequences [2, 19]. In all bu"}, {"source": "test.pdf", "text": "In all but a few cases [27], however, such attent"}, {"source": "test.pdf", "text": "uch attention mechanisms\nare used in conjunction w"}, {"source": "test.pdf", "text": "junction with a recurrent network.\nIn this work we"}, {"source": "test.pdf", "text": "is work we propose the Transformer, a model archit"}, {"source": "test.pdf", "text": "del architecture eschewing recurrence and instead"}, {"source": "test.pdf", "text": "d instead\nrelying entirely on an attention mechani"}, {"source": "test.pdf", "text": "on mechanism to draw global dependencies between i"}, {"source": "test.pdf", "text": "between input and output.\nThe Transformer allows"}, {"source": "test.pdf", "text": "er allows for significantly more parallelization a"}, {"source": "test.pdf", "text": "lization and can reach a new state of the art in\nt"}, {"source": "test.pdf", "text": "e art in\ntranslation quality after being trained f"}, {"source": "test.pdf", "text": "trained for as little as twelve hours on eight P1"}, {"source": "test.pdf", "text": "n eight P100 GPUs.\n2\nBackground\nThe goal of reduci"}, {"source": "test.pdf", "text": "of reducing sequential computation also forms the"}, {"source": "test.pdf", "text": "forms the foundation of the Extended Neural GPU\n["}, {"source": "test.pdf", "text": "ural GPU\n[16], ByteNet [18] and ConvS2S [9], all o"}, {"source": "test.pdf", "text": "[9], all of which use convolutional neural network"}, {"source": "test.pdf", "text": "al networks as basic building\nblock, computing hid"}, {"source": "test.pdf", "text": "puting hidden representations in parallel for all"}, {"source": "test.pdf", "text": "l for all input and output positions. In these mod"}, {"source": "test.pdf", "text": "these models,\nthe number of operations required t"}, {"source": "test.pdf", "text": "required to relate signals from two arbitrary inpu"}, {"source": "test.pdf", "text": "trary input or output positions grows\nin the dista"}, {"source": "test.pdf", "text": "the distance between positions, linearly for Conv"}, {"source": "test.pdf", "text": "y for ConvS2S and logarithmically for ByteNet. Thi"}, {"source": "test.pdf", "text": "teNet. This makes\nit more difficult to learn depen"}, {"source": "test.pdf", "text": "earn dependencies between distant positions [12]."}, {"source": "test.pdf", "text": "ons [12]. In the Transformer this is\nreduced to a"}, {"source": "test.pdf", "text": "uced to a constant number of operations, albeit at"}, {"source": "test.pdf", "text": "albeit at the cost of reduced effective resolutio"}, {"source": "test.pdf", "text": "resolution due\nto averaging attention-weighted po"}, {"source": "test.pdf", "text": "eighted positions, an effect we counteract with Mu"}, {"source": "test.pdf", "text": "ct with Multi-Head Attention as\ndescribed in secti"}, {"source": "test.pdf", "text": "d in section 3.2.\nSelf-attention, sometimes called"}, {"source": "test.pdf", "text": "mes called intra-attention is an attention mechani"}, {"source": "test.pdf", "text": "on mechanism relating different positions\nof a sin"}, {"source": "test.pdf", "text": "s\nof a single sequence in order to compute a repre"}, {"source": "test.pdf", "text": "te a representation of the sequence. Self-attentio"}, {"source": "test.pdf", "text": "f-attention has been\nused successfully in a variet"}, {"source": "test.pdf", "text": "n a variety of tasks including reading comprehensi"}, {"source": "test.pdf", "text": "omprehension, abstractive summarization,\ntextual e"}, {"source": "test.pdf", "text": "textual entailment and learning task-independent"}, {"source": "test.pdf", "text": "dependent sentence representations [4, 27, 28, 22]"}, {"source": "test.pdf", "text": "7, 28, 22].\nEnd-to-end memory networks are based o"}, {"source": "test.pdf", "text": "re based on a recurrent attention mechanism instea"}, {"source": "test.pdf", "text": "ism instead of sequence-\naligned recurrence and ha"}, {"source": "test.pdf", "text": "nce and have been shown to perform well on simple-"}, {"source": "test.pdf", "text": "on simple-language question answering and\nlanguage"}, {"source": "test.pdf", "text": "d\nlanguage modeling tasks [34].\nTo the best of our"}, {"source": "test.pdf", "text": "est of our knowledge, however, the Transformer is"}, {"source": "test.pdf", "text": "former is the first transduction model relying\nent"}, {"source": "test.pdf", "text": "elying\nentirely on self-attention to compute repre"}, {"source": "test.pdf", "text": "pute representations of its input and output witho"}, {"source": "test.pdf", "text": "tput without using sequence-\naligned RNNs or convo"}, {"source": "test.pdf", "text": "s or convolution. In the following sections, we wi"}, {"source": "test.pdf", "text": "ons, we will describe the Transformer, motivate\nse"}, {"source": "test.pdf", "text": "otivate\nself-attention and discuss its advantages"}, {"source": "test.pdf", "text": "dvantages over models such as [17, 18] and [9].\n3"}, {"source": "test.pdf", "text": "nd [9].\n3\nModel Architecture\nMost competitive neur"}, {"source": "test.pdf", "text": "itive neural sequence transduction models have an"}, {"source": "test.pdf", "text": "s have an encoder-decoder structure [5, 2, 35].\nHe"}, {"source": "test.pdf", "text": "2, 35].\nHere, the encoder maps an input sequence o"}, {"source": "test.pdf", "text": "sequence of symbol representations (x1, ..., xn) t"}, {"source": "test.pdf", "text": "..., xn) to a sequence\nof continuous representatio"}, {"source": "test.pdf", "text": "resentations z = (z1, ..., zn). Given z, the decod"}, {"source": "test.pdf", "text": "the decoder then generates an output\nsequence (y1"}, {"source": "test.pdf", "text": "quence (y1, ..., ym) of symbols one element at a t"}, {"source": "test.pdf", "text": "ent at a time. At each step the model is auto-regr"}, {"source": "test.pdf", "text": "auto-regressive\n[10], consuming the previously ge"}, {"source": "test.pdf", "text": "viously generated symbols as additional input when"}, {"source": "test.pdf", "text": "input when generating the next.\n2\nFigure 1: The Tr"}, {"source": "test.pdf", "text": "1: The Transformer - model architecture.\nThe Tran"}, {"source": "test.pdf", "text": ".\nThe Transformer follows this overall architectur"}, {"source": "test.pdf", "text": "rchitecture using stacked self-attention and point"}, {"source": "test.pdf", "text": "and point-wise, fully\nconnected layers for both t"}, {"source": "test.pdf", "text": "for both the encoder and decoder, shown in the lef"}, {"source": "test.pdf", "text": "in the left and right halves of Figure 1,\nrespecti"}, {"source": "test.pdf", "text": ",\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEnc"}, {"source": "test.pdf", "text": "Stacks\nEncoder:\nThe encoder is composed of a stack"}, {"source": "test.pdf", "text": "of a stack of N = 6 identical layers. Each layer h"}, {"source": "test.pdf", "text": "ch layer has two\nsub-layers. The first is a multi-"}, {"source": "test.pdf", "text": "s a multi-head self-attention mechanism, and the s"}, {"source": "test.pdf", "text": "and the second is a simple, position-\nwise fully"}, {"source": "test.pdf", "text": "ise fully connected feed-forward network. We emplo"}, {"source": "test.pdf", "text": ". We employ a residual connection [11] around each"}, {"source": "test.pdf", "text": "round each of\nthe two sub-layers, followed by laye"}, {"source": "test.pdf", "text": "ed by layer normalization [1]. That is, the output"}, {"source": "test.pdf", "text": "the output of each sub-layer is\nLayerNorm(x + Subl"}, {"source": "test.pdf", "text": "m(x + Sublayer(x)), where Sublayer(x) is the funct"}, {"source": "test.pdf", "text": "the function implemented by the sub-layer\nitself."}, {"source": "test.pdf", "text": "er\nitself. To facilitate these residual connection"}, {"source": "test.pdf", "text": "connections, all sub-layers in the model, as well"}, {"source": "test.pdf", "text": ", as well as the embedding\nlayers, produce outputs"}, {"source": "test.pdf", "text": "ce outputs of dimension dmodel = 512.\nDecoder:\nThe"}, {"source": "test.pdf", "text": "coder:\nThe decoder is also composed of a stack of"}, {"source": "test.pdf", "text": "stack of N = 6 identical layers. In addition to t"}, {"source": "test.pdf", "text": "ition to the two\nsub-layers in each encoder layer,"}, {"source": "test.pdf", "text": "der layer, the decoder inserts a third sub-layer,"}, {"source": "test.pdf", "text": "ub-layer, which performs multi-head\nattention over"}, {"source": "test.pdf", "text": "ntion over the output of the encoder stack. Simila"}, {"source": "test.pdf", "text": "ck. Similar to the encoder, we employ residual con"}, {"source": "test.pdf", "text": "sidual connections\naround each of the sub-layers,"}, {"source": "test.pdf", "text": "b-layers, followed by layer normalization. We also"}, {"source": "test.pdf", "text": "n. We also modify the self-attention\nsub-layer in"}, {"source": "test.pdf", "text": "-layer in the decoder stack to prevent positions f"}, {"source": "test.pdf", "text": "ositions from attending to subsequent positions. T"}, {"source": "test.pdf", "text": "sitions. This\nmasking, combined with fact that the"}, {"source": "test.pdf", "text": "t that the output embeddings are offset by one pos"}, {"source": "test.pdf", "text": "by one position, ensures that the\npredictions for"}, {"source": "test.pdf", "text": "tions for position i can depend only on the known"}, {"source": "test.pdf", "text": "the known outputs at positions less than i.\n3.2\nAt"}, {"source": "test.pdf", "text": "i.\n3.2\nAttention\nAn attention function can be des"}, {"source": "test.pdf", "text": "can be described as mapping a query and a set of k"}, {"source": "test.pdf", "text": "a set of key-value pairs to an output,\nwhere the q"}, {"source": "test.pdf", "text": "here the query, keys, values, and output are all v"}, {"source": "test.pdf", "text": "are all vectors. The output is computed as a weig"}, {"source": "test.pdf", "text": "as a weighted sum\n3\nScaled Dot-Product Attention"}, {"source": "test.pdf", "text": "Attention\nMulti-Head Attention\nFigure 2: (left) Sc"}, {"source": "test.pdf", "text": "(left) Scaled Dot-Product Attention. (right) Mult"}, {"source": "test.pdf", "text": "ight) Multi-Head Attention consists of several\natt"}, {"source": "test.pdf", "text": "everal\nattention layers running in parallel.\nof th"}, {"source": "test.pdf", "text": "lel.\nof the values, where the weight assigned to e"}, {"source": "test.pdf", "text": "igned to each value is computed by a compatibility"}, {"source": "test.pdf", "text": "patibility function of the\nquery with the correspo"}, {"source": "test.pdf", "text": "e corresponding key.\n3.2.1\nScaled Dot-Product Atte"}, {"source": "test.pdf", "text": "oduct Attention\nWe call our particular attention \""}, {"source": "test.pdf", "text": "ttention \"Scaled Dot-Product Attention\" (Figure 2)"}, {"source": "test.pdf", "text": "(Figure 2). The input consists of\nqueries and keys"}, {"source": "test.pdf", "text": "s and keys of dimension dk, and values of dimensio"}, {"source": "test.pdf", "text": "f dimension dv. We compute the dot products of the"}, {"source": "test.pdf", "text": "cts of the\nquery with all keys, divide each by √dk"}, {"source": "test.pdf", "text": "ach by √dk, and apply a softmax function to obtain"}, {"source": "test.pdf", "text": "to obtain the weights on the\nvalues.\nIn practice,"}, {"source": "test.pdf", "text": "practice, we compute the attention function on a"}, {"source": "test.pdf", "text": "tion on a set of queries simultaneously, packed to"}, {"source": "test.pdf", "text": "packed together\ninto a matrix Q. The keys and val"}, {"source": "test.pdf", "text": "ys and values are also packed together into matric"}, {"source": "test.pdf", "text": "nto matrices K and V . We compute\nthe matrix of ou"}, {"source": "test.pdf", "text": "trix of outputs as:\nAttention(Q, K, V ) = softmax("}, {"source": "test.pdf", "text": "= softmax(QKT\n√dk\n)V\n(1)\nThe two most commonly use"}, {"source": "test.pdf", "text": "mmonly used attention functions are additive atten"}, {"source": "test.pdf", "text": "tive attention [2], and dot-product (multi-\nplicat"}, {"source": "test.pdf", "text": "ti-\nplicative) attention. Dot-product attention is"}, {"source": "test.pdf", "text": "tention is identical to our algorithm, except for"}, {"source": "test.pdf", "text": "xcept for the scaling factor\nof\n1\n√dk . Additive a"}, {"source": "test.pdf", "text": "Additive attention computes the compatibility func"}, {"source": "test.pdf", "text": "ility function using a feed-forward network with\na"}, {"source": "test.pdf", "text": "ork with\na single hidden layer. While the two are"}, {"source": "test.pdf", "text": "e two are similar in theoretical complexity, dot-p"}, {"source": "test.pdf", "text": "ity, dot-product attention is\nmuch faster and more"}, {"source": "test.pdf", "text": "r and more space-efficient in practice, since it c"}, {"source": "test.pdf", "text": "since it can be implemented using highly optimized"}, {"source": "test.pdf", "text": "optimized\nmatrix multiplication code.\nWhile for s"}, {"source": "test.pdf", "text": "hile for small values of dk the two mechanisms per"}, {"source": "test.pdf", "text": "anisms perform similarly, additive attention outpe"}, {"source": "test.pdf", "text": "tion outperforms\ndot product attention without sca"}, {"source": "test.pdf", "text": "ithout scaling for larger values of dk [3]. We sus"}, {"source": "test.pdf", "text": "3]. We suspect that for large values of\ndk, the do"}, {"source": "test.pdf", "text": "dk, the dot products grow large in magnitude, push"}, {"source": "test.pdf", "text": "tude, pushing the softmax function into regions wh"}, {"source": "test.pdf", "text": "regions where it has\nextremely small gradients 4."}, {"source": "test.pdf", "text": "dients 4. To counteract this effect, we scale the"}, {"source": "test.pdf", "text": "scale the dot products by\n1\n√dk .\n3.2.2\nMulti-Head"}, {"source": "test.pdf", "text": "Multi-Head Attention\nInstead of performing a singl"}, {"source": "test.pdf", "text": "ng a single attention function with dmodel-dimensi"}, {"source": "test.pdf", "text": "el-dimensional keys, values and queries,\nwe found"}, {"source": "test.pdf", "text": "we found it beneficial to linearly project the qu"}, {"source": "test.pdf", "text": "ect the queries, keys and values h times with diff"}, {"source": "test.pdf", "text": "with different, learned\nlinear projections to dk,"}, {"source": "test.pdf", "text": "ons to dk, dk and dv dimensions, respectively. On"}, {"source": "test.pdf", "text": "ively. On each of these projected versions of\nquer"}, {"source": "test.pdf", "text": "ns of\nqueries, keys and values we then perform the"}, {"source": "test.pdf", "text": "erform the attention function in parallel, yieldin"}, {"source": "test.pdf", "text": "l, yielding dv-dimensional\n4To illustrate why the"}, {"source": "test.pdf", "text": "e why the dot products get large, assume that the"}, {"source": "test.pdf", "text": "that the components of q and k are independent ra"}, {"source": "test.pdf", "text": "pendent random\nvariables with mean 0 and variance"}, {"source": "test.pdf", "text": "variance 1. Then their dot product, q · k = Pdk\ni"}, {"source": "test.pdf", "text": "k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4\no"}, {"source": "test.pdf", "text": "ce dk.\n4\noutput values. These are concatenated and"}, {"source": "test.pdf", "text": "enated and once again projected, resulting in the"}, {"source": "test.pdf", "text": "ng in the final values, as\ndepicted in Figure 2.\nM"}, {"source": "test.pdf", "text": "igure 2.\nMulti-head attention allows the model to"}, {"source": "test.pdf", "text": "model to jointly attend to information from diffe"}, {"source": "test.pdf", "text": "from different representation\nsubspaces at differe"}, {"source": "test.pdf", "text": "at different positions. With a single attention he"}, {"source": "test.pdf", "text": "tention head, averaging inhibits this.\nMultiHead(Q"}, {"source": "test.pdf", "text": "ultiHead(Q, K, V ) = Concat(head1, ..., headh)W O"}, {"source": "test.pdf", "text": "headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni"}, {"source": "test.pdf", "text": ", KW K\ni , V W V\ni )\nWhere the projections are pa"}, {"source": "test.pdf", "text": "ons are parameter matrices W Q\ni\n∈Rdmodel×dk, W K"}, {"source": "test.pdf", "text": "l×dk, W K\ni\n∈Rdmodel×dk, W V\ni\n∈Rdmodel×dv\nand W O"}, {"source": "test.pdf", "text": "dv\nand W O ∈Rhdv×dmodel.\nIn this work we employ h"}, {"source": "test.pdf", "text": "employ h = 8 parallel attention layers, or heads."}, {"source": "test.pdf", "text": "or heads. For each of these we use\ndk = dv = dmod"}, {"source": "test.pdf", "text": "dv = dmodel/h = 64. Due to the reduced dimension"}, {"source": "test.pdf", "text": "dimension of each head, the total computational co"}, {"source": "test.pdf", "text": "ational cost\nis similar to that of single-head att"}, {"source": "test.pdf", "text": "e-head attention with full dimensionality.\n3.2.3\nA"}, {"source": "test.pdf", "text": "y.\n3.2.3\nApplications of Attention in our Model\nTh"}, {"source": "test.pdf", "text": "r Model\nThe Transformer uses multi-head attention"}, {"source": "test.pdf", "text": "attention in three different ways:\n• In \"encoder-d"}, {"source": "test.pdf", "text": "\"encoder-decoder attention\" layers, the queries co"}, {"source": "test.pdf", "text": "queries come from the previous decoder layer,\nand"}, {"source": "test.pdf", "text": "ayer,\nand the memory keys and values come from the"}, {"source": "test.pdf", "text": "e from the output of the encoder. This allows ever"}, {"source": "test.pdf", "text": "llows every\nposition in the decoder to attend over"}, {"source": "test.pdf", "text": "ttend over all positions in the input sequence. Th"}, {"source": "test.pdf", "text": "quence. This mimics the\ntypical encoder-decoder at"}, {"source": "test.pdf", "text": "decoder attention mechanisms in sequence-to-sequen"}, {"source": "test.pdf", "text": "-to-sequence models such as\n[38, 2, 9].\n• The enco"}, {"source": "test.pdf", "text": "• The encoder contains self-attention layers. In a"}, {"source": "test.pdf", "text": "yers. In a self-attention layer all of the keys, v"}, {"source": "test.pdf", "text": "he keys, values\nand queries come from the same pla"}, {"source": "test.pdf", "text": "e same place, in this case, the output of the prev"}, {"source": "test.pdf", "text": "f the previous layer in the\nencoder. Each position"}, {"source": "test.pdf", "text": "h position in the encoder can attend to all positi"}, {"source": "test.pdf", "text": "all positions in the previous layer of the\nencoder"}, {"source": "test.pdf", "text": "he\nencoder.\n• Similarly, self-attention layers in"}, {"source": "test.pdf", "text": "layers in the decoder allow each position in the d"}, {"source": "test.pdf", "text": "n in the decoder to attend to\nall positions in the"}, {"source": "test.pdf", "text": "ons in the decoder up to and including that positi"}, {"source": "test.pdf", "text": "hat position. We need to prevent leftward\ninformat"}, {"source": "test.pdf", "text": "d\ninformation flow in the decoder to preserve the"}, {"source": "test.pdf", "text": "serve the auto-regressive property. We implement t"}, {"source": "test.pdf", "text": "mplement this\ninside of scaled dot-product attenti"}, {"source": "test.pdf", "text": "ct attention by masking out (setting to −∞) all va"}, {"source": "test.pdf", "text": "−∞) all values in the input\nof the softmax which c"}, {"source": "test.pdf", "text": "ax which correspond to illegal connections. See Fi"}, {"source": "test.pdf", "text": "ns. See Figure 2.\n3.3\nPosition-wise Feed-Forward N"}, {"source": "test.pdf", "text": "-Forward Networks\nIn addition to attention sub-lay"}, {"source": "test.pdf", "text": "on sub-layers, each of the layers in our encoder a"}, {"source": "test.pdf", "text": "encoder and decoder contains a fully\nconnected fe"}, {"source": "test.pdf", "text": "nnected feed-forward network, which is applied to"}, {"source": "test.pdf", "text": "pplied to each position separately and identically"}, {"source": "test.pdf", "text": "dentically. This\nconsists of two linear transforma"}, {"source": "test.pdf", "text": "transformations with a ReLU activation in between."}, {"source": "test.pdf", "text": "n between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWh"}, {"source": "test.pdf", "text": "b2\n(2)\nWhile the linear transformations are the s"}, {"source": "test.pdf", "text": "are the same across different positions, they use"}, {"source": "test.pdf", "text": ", they use different parameters\nfrom layer to laye"}, {"source": "test.pdf", "text": "er to layer. Another way of describing this is as"}, {"source": "test.pdf", "text": "his is as two convolutions with kernel size 1.\nThe"}, {"source": "test.pdf", "text": "ize 1.\nThe dimensionality of input and output is d"}, {"source": "test.pdf", "text": "utput is dmodel = 512, and the inner-layer has dim"}, {"source": "test.pdf", "text": "er has dimensionality\ndff = 2048.\n3.4\nEmbeddings a"}, {"source": "test.pdf", "text": "beddings and Softmax\nSimilarly to other sequence t"}, {"source": "test.pdf", "text": "sequence transduction models, we use learned embed"}, {"source": "test.pdf", "text": "rned embeddings to convert the input\ntokens and ou"}, {"source": "test.pdf", "text": "ens and output tokens to vectors of dimension dmod"}, {"source": "test.pdf", "text": "nsion dmodel. We also use the usual learned linear"}, {"source": "test.pdf", "text": "ned linear transfor-\nmation and softmax function t"}, {"source": "test.pdf", "text": "function to convert the decoder output to predicte"}, {"source": "test.pdf", "text": "o predicted next-token probabilities. In\nour model"}, {"source": "test.pdf", "text": "our model, we share the same weight matrix betwee"}, {"source": "test.pdf", "text": "rix between the two embedding layers and the pre-s"}, {"source": "test.pdf", "text": "the pre-softmax\nlinear transformation, similar to"}, {"source": "test.pdf", "text": "similar to [30]. In the embedding layers, we multi"}, {"source": "test.pdf", "text": ", we multiply those weights by √dmodel.\n5\nTable 1:"}, {"source": "test.pdf", "text": "5\nTable 1: Maximum path lengths, per-layer complex"}, {"source": "test.pdf", "text": "er complexity and minimum number of sequential ope"}, {"source": "test.pdf", "text": "ential operations\nfor different layer types. n is"}, {"source": "test.pdf", "text": "pes. n is the sequence length, d is the representa"}, {"source": "test.pdf", "text": "representation dimension, k is the kernel\nsize of"}, {"source": "test.pdf", "text": "l\nsize of convolutions and r the size of the neigh"}, {"source": "test.pdf", "text": "the neighborhood in restricted self-attention.\nLa"}, {"source": "test.pdf", "text": "ention.\nLayer Type\nComplexity per Layer\nSequential"}, {"source": "test.pdf", "text": "Sequential\nMaximum Path Length\nOperations\nSelf-Att"}, {"source": "test.pdf", "text": "s\nSelf-Attention\nO(n2 · d)\nO(1)\nO(1)\nRecurrent\nO(n"}, {"source": "test.pdf", "text": "urrent\nO(n · d2)\nO(n)\nO(n)\nConvolutional\nO(k · n ·"}, {"source": "test.pdf", "text": "O(k · n · d2)\nO(1)\nO(logk(n))\nSelf-Attention (res"}, {"source": "test.pdf", "text": "ntion (restricted)\nO(r · n · d)\nO(1)\nO(n/r)\n3.5\nPo"}, {"source": "test.pdf", "text": "/r)\n3.5\nPositional Encoding\nSince our model contai"}, {"source": "test.pdf", "text": "del contains no recurrence and no convolution, in"}, {"source": "test.pdf", "text": "ution, in order for the model to make use of the\no"}, {"source": "test.pdf", "text": "e of the\norder of the sequence, we must inject som"}, {"source": "test.pdf", "text": "inject some information about the relative or abso"}, {"source": "test.pdf", "text": "ve or absolute position of the\ntokens in the seque"}, {"source": "test.pdf", "text": "the sequence. To this end, we add \"positional enc"}, {"source": "test.pdf", "text": "tional encodings\" to the input embeddings at the\nb"}, {"source": "test.pdf", "text": "s at the\nbottoms of the encoder and decoder stacks"}, {"source": "test.pdf", "text": "der stacks. The positional encodings have the same"}, {"source": "test.pdf", "text": "e the same dimension dmodel\nas the embeddings, so"}, {"source": "test.pdf", "text": "dings, so that the two can be summed. There are ma"}, {"source": "test.pdf", "text": "ere are many choices of positional encodings,\nlear"}, {"source": "test.pdf", "text": "ings,\nlearned and fixed [9].\nIn this work, we use"}, {"source": "test.pdf", "text": "k, we use sine and cosine functions of different f"}, {"source": "test.pdf", "text": "ifferent frequencies:\nPE(pos,2i) = sin(pos/100002i"}, {"source": "test.pdf", "text": "os/100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/"}, {"source": "test.pdf", "text": "s/100002i/dmodel)\nwhere pos is the position and i"}, {"source": "test.pdf", "text": "ion and i is the dimension. That is, each dimensio"}, {"source": "test.pdf", "text": "h dimension of the positional encoding\ncorresponds"}, {"source": "test.pdf", "text": "orresponds to a sinusoid. The wavelengths form a g"}, {"source": "test.pdf", "text": "s form a geometric progression from 2π to 10000 ·"}, {"source": "test.pdf", "text": "o 10000 · 2π. We\nchose this function because we hy"}, {"source": "test.pdf", "text": "ause we hypothesized it would allow the model to e"}, {"source": "test.pdf", "text": "model to easily learn to attend by\nrelative positi"}, {"source": "test.pdf", "text": "ive positions, since for any fixed offset k, PEpos"}, {"source": "test.pdf", "text": "t k, PEpos+k can be represented as a linear functi"}, {"source": "test.pdf", "text": "ear function of\nPEpos.\nWe also experimented with u"}, {"source": "test.pdf", "text": "ted with using learned positional embeddings [9] i"}, {"source": "test.pdf", "text": "ings [9] instead, and found that the two\nversions"}, {"source": "test.pdf", "text": "versions produced nearly identical results (see T"}, {"source": "test.pdf", "text": "lts (see Table 3 row (E)). We chose the sinusoidal"}, {"source": "test.pdf", "text": "sinusoidal version\nbecause it may allow the model"}, {"source": "test.pdf", "text": "the model to extrapolate to sequence lengths longe"}, {"source": "test.pdf", "text": "gths longer than the ones encountered\nduring train"}, {"source": "test.pdf", "text": "ring training.\n4\nWhy Self-Attention\nIn this sectio"}, {"source": "test.pdf", "text": "his section we compare various aspects of self-att"}, {"source": "test.pdf", "text": "f self-attention layers to the recurrent and convo"}, {"source": "test.pdf", "text": "and convolu-\ntional layers commonly used for mapp"}, {"source": "test.pdf", "text": "d for mapping one variable-length sequence of symb"}, {"source": "test.pdf", "text": "ce of symbol representations\n(x1, ..., xn) to anot"}, {"source": "test.pdf", "text": "n) to another sequence of equal length (z1, ..., z"}, {"source": "test.pdf", "text": "z1, ..., zn), with xi, zi ∈Rd, such as a hidden\nla"}, {"source": "test.pdf", "text": "hidden\nlayer in a typical sequence transduction e"}, {"source": "test.pdf", "text": "sduction encoder or decoder. Motivating our use of"}, {"source": "test.pdf", "text": "our use of self-attention we\nconsider three deside"}, {"source": "test.pdf", "text": "ree desiderata.\nOne is the total computational com"}, {"source": "test.pdf", "text": "tional complexity per layer. Another is the amount"}, {"source": "test.pdf", "text": "the amount of computation that can\nbe parallelized"}, {"source": "test.pdf", "text": "rallelized, as measured by the minimum number of s"}, {"source": "test.pdf", "text": "umber of sequential operations required.\nThe third"}, {"source": "test.pdf", "text": "The third is the path length between long-range d"}, {"source": "test.pdf", "text": "ng-range dependencies in the network. Learning lon"}, {"source": "test.pdf", "text": "arning long-range\ndependencies is a key challenge"}, {"source": "test.pdf", "text": "challenge in many sequence transduction tasks. One"}, {"source": "test.pdf", "text": "tasks. One key factor affecting the\nability to lea"}, {"source": "test.pdf", "text": "ity to learn such dependencies is the length of th"}, {"source": "test.pdf", "text": "ngth of the paths forward and backward signals hav"}, {"source": "test.pdf", "text": "ignals have to\ntraverse in the network. The shorte"}, {"source": "test.pdf", "text": "The shorter these paths between any combination of"}, {"source": "test.pdf", "text": "ination of positions in the input\nand output seque"}, {"source": "test.pdf", "text": "tput sequences, the easier it is to learn long-ran"}, {"source": "test.pdf", "text": "n long-range dependencies [12]. Hence we also comp"}, {"source": "test.pdf", "text": "also compare\nthe maximum path length between any"}, {"source": "test.pdf", "text": "tween any two input and output positions in networ"}, {"source": "test.pdf", "text": "in networks composed of the\ndifferent layer types"}, {"source": "test.pdf", "text": "ayer types.\nAs noted in Table 1, a self-attention"}, {"source": "test.pdf", "text": "attention layer connects all positions with a cons"}, {"source": "test.pdf", "text": "ith a constant number of sequentially\nexecuted ope"}, {"source": "test.pdf", "text": "ecuted operations, whereas a recurrent layer requi"}, {"source": "test.pdf", "text": "ayer requires O(n) sequential operations. In terms"}, {"source": "test.pdf", "text": ". In terms of\ncomputational complexity, self-atten"}, {"source": "test.pdf", "text": "self-attention layers are faster than recurrent la"}, {"source": "test.pdf", "text": "current layers when the sequence\n6\nlength n is sma"}, {"source": "test.pdf", "text": "h n is smaller than the representation dimensional"}, {"source": "test.pdf", "text": "imensionality d, which is most often the case with"}, {"source": "test.pdf", "text": "case with\nsentence representations used by state-"}, {"source": "test.pdf", "text": "by state-of-the-art models in machine translation"}, {"source": "test.pdf", "text": "ranslations, such as word-piece\n[38] and byte-pair"}, {"source": "test.pdf", "text": "byte-pair [31] representations. To improve comput"}, {"source": "test.pdf", "text": "ove computational performance for tasks involving"}, {"source": "test.pdf", "text": "involving\nvery long sequences, self-attention coul"}, {"source": "test.pdf", "text": "ntion could be restricted to considering only a ne"}, {"source": "test.pdf", "text": "only a neighborhood of size r in\nthe input sequen"}, {"source": "test.pdf", "text": "put sequence centered around the respective output"}, {"source": "test.pdf", "text": "ive output position. This would increase the maxim"}, {"source": "test.pdf", "text": "the maximum\npath length to O(n/r). We plan to inv"}, {"source": "test.pdf", "text": "lan to investigate this approach further in future"}, {"source": "test.pdf", "text": "in future work.\nA single convolutional layer with"}, {"source": "test.pdf", "text": "layer with kernel width k < n does not connect all"}, {"source": "test.pdf", "text": "onnect all pairs of input and output\npositions. Do"}, {"source": "test.pdf", "text": "itions. Doing so requires a stack of O(n/k) convol"}, {"source": "test.pdf", "text": "/k) convolutional layers in the case of contiguous"}, {"source": "test.pdf", "text": "contiguous kernels,\nor O(logk(n)) in the case of d"}, {"source": "test.pdf", "text": "case of dilated convolutions [18], increasing the"}, {"source": "test.pdf", "text": "easing the length of the longest paths\nbetween any"}, {"source": "test.pdf", "text": "etween any two positions in the network. Convoluti"}, {"source": "test.pdf", "text": "Convolutional layers are generally more expensive"}, {"source": "test.pdf", "text": "expensive than\nrecurrent layers, by a factor of k"}, {"source": "test.pdf", "text": "actor of k. Separable convolutions [6], however, d"}, {"source": "test.pdf", "text": "however, decrease the complexity\nconsiderably, to"}, {"source": "test.pdf", "text": "rably, to O(k · n · d + n · d2). Even with k = n,"}, {"source": "test.pdf", "text": "th k = n, however, the complexity of a separable\nc"}, {"source": "test.pdf", "text": "eparable\nconvolution is equal to the combination o"}, {"source": "test.pdf", "text": "bination of a self-attention layer and a point-wis"}, {"source": "test.pdf", "text": "point-wise feed-forward layer,\nthe approach we ta"}, {"source": "test.pdf", "text": "oach we take in our model.\nAs side benefit, self-a"}, {"source": "test.pdf", "text": "it, self-attention could yield more interpretable"}, {"source": "test.pdf", "text": "rpretable models. We inspect attention distributio"}, {"source": "test.pdf", "text": "istributions\nfrom our models and present and discu"}, {"source": "test.pdf", "text": "and discuss examples in the appendix. Not only do"}, {"source": "test.pdf", "text": "ot only do individual attention\nheads clearly lear"}, {"source": "test.pdf", "text": "early learn to perform different tasks, many appea"}, {"source": "test.pdf", "text": "many appear to exhibit behavior related to the syn"}, {"source": "test.pdf", "text": "to the syntactic\nand semantic structure of the sen"}, {"source": "test.pdf", "text": "of the sentences.\n5\nTraining\nThis section describe"}, {"source": "test.pdf", "text": "n describes the training regime for our models.\n5."}, {"source": "test.pdf", "text": "models.\n5.1\nTraining Data and Batching\nWe trained"}, {"source": "test.pdf", "text": "e trained on the standard WMT 2014 English-German"}, {"source": "test.pdf", "text": "sh-German dataset consisting of about 4.5 million"}, {"source": "test.pdf", "text": "5 million\nsentence pairs. Sentences were encoded u"}, {"source": "test.pdf", "text": "encoded using byte-pair encoding [3], which has a"}, {"source": "test.pdf", "text": "hich has a shared source-\ntarget vocabulary of abo"}, {"source": "test.pdf", "text": "ary of about 37000 tokens. For English-French, we"}, {"source": "test.pdf", "text": "rench, we used the significantly larger WMT\n2014 E"}, {"source": "test.pdf", "text": "WMT\n2014 English-French dataset consisting of 36M"}, {"source": "test.pdf", "text": "ng of 36M sentences and split tokens into a 32000"}, {"source": "test.pdf", "text": "o a 32000 word-piece\nvocabulary [38]. Sentence pai"}, {"source": "test.pdf", "text": "ntence pairs were batched together by approximate"}, {"source": "test.pdf", "text": "proximate sequence length. Each training\nbatch con"}, {"source": "test.pdf", "text": "batch contained a set of sentence pairs containin"}, {"source": "test.pdf", "text": "containing approximately 25000 source tokens and"}, {"source": "test.pdf", "text": "okens and 25000\ntarget tokens.\n5.2\nHardware and Sc"}, {"source": "test.pdf", "text": "are and Schedule\nWe trained our models on one mach"}, {"source": "test.pdf", "text": "n one machine with 8 NVIDIA P100 GPUs. For our bas"}, {"source": "test.pdf", "text": "or our base models using\nthe hyperparameters descr"}, {"source": "test.pdf", "text": "ters described throughout the paper, each training"}, {"source": "test.pdf", "text": "h training step took about 0.4 seconds. We\ntrained"}, {"source": "test.pdf", "text": "We\ntrained the base models for a total of 100,000"}, {"source": "test.pdf", "text": "f 100,000 steps or 12 hours. For our big models,(d"}, {"source": "test.pdf", "text": "models,(described on the\nbottom line of table 3),"}, {"source": "test.pdf", "text": "table 3), step time was 1.0 seconds. The big mode"}, {"source": "test.pdf", "text": "e big models were trained for 300,000 steps\n(3.5 d"}, {"source": "test.pdf", "text": "eps\n(3.5 days).\n5.3\nOptimizer\nWe used the Adam opt"}, {"source": "test.pdf", "text": "e Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and"}, {"source": "test.pdf", "text": "= 0.98 and ϵ = 10−9. We varied the learning\nrate o"}, {"source": "test.pdf", "text": "ing\nrate over the course of training, according to"}, {"source": "test.pdf", "text": "cording to the formula:\nlrate = d−0.5\nmodel · min("}, {"source": "test.pdf", "text": "del · min(step_num−0.5, step_num · warmup_steps−1."}, {"source": "test.pdf", "text": "p_steps−1.5)\n(3)\nThis corresponds to increasing th"}, {"source": "test.pdf", "text": "reasing the learning rate linearly for the first w"}, {"source": "test.pdf", "text": "he first warmup_steps training steps,\nand decreasi"}, {"source": "test.pdf", "text": "d decreasing it thereafter proportionally to the i"}, {"source": "test.pdf", "text": "y to the inverse square root of the step number. W"}, {"source": "test.pdf", "text": "number. We used\nwarmup_steps = 4000.\n5.4\nRegulari"}, {"source": "test.pdf", "text": "4\nRegularization\nWe employ three types of regulari"}, {"source": "test.pdf", "text": "f regularization during training:\n7\nTable 2: The T"}, {"source": "test.pdf", "text": "e 2: The Transformer achieves better BLEU scores t"}, {"source": "test.pdf", "text": "U scores than previous state-of-the-art models on"}, {"source": "test.pdf", "text": "models on the\nEnglish-to-German and English-to-Fre"}, {"source": "test.pdf", "text": "ish-to-French newstest2014 tests at a fraction of"}, {"source": "test.pdf", "text": "action of the training cost.\nModel\nBLEU\nTraining C"}, {"source": "test.pdf", "text": "Training Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByte"}, {"source": "test.pdf", "text": "EN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39"}, {"source": "test.pdf", "text": "nk [39]\n39.2\n1.0 · 1020\nGNMT + RL [38]\n24.6\n39.92"}, {"source": "test.pdf", "text": "4.6\n39.92\n2.3 · 1019\n1.4 · 1020\nConvS2S [9]\n25.16"}, {"source": "test.pdf", "text": "[9]\n25.16\n40.46\n9.6 · 1018\n1.5 · 1020\nMoE [32]\n26."}, {"source": "test.pdf", "text": "E [32]\n26.03\n40.56\n2.0 · 1019\n1.2 · 1020\nDeep-Att"}, {"source": "test.pdf", "text": "Deep-Att + PosUnk Ensemble [39]\n40.4\n8.0 · 1020\nG"}, {"source": "test.pdf", "text": "0 · 1020\nGNMT + RL Ensemble [38]\n26.30\n41.16\n1.8 ·"}, {"source": "test.pdf", "text": "1.16\n1.8 · 1020\n1.1 · 1021\nConvS2S Ensemble [9]\n26"}, {"source": "test.pdf", "text": "ble [9]\n26.36\n41.29\n7.7 · 1019\n1.2 · 1021\nTransfor"}, {"source": "test.pdf", "text": "1\nTransformer (base model)\n27.3\n38.1\n3.3 · 1018\nTr"}, {"source": "test.pdf", "text": "· 1018\nTransformer (big)\n28.4\n41.8\n2.3 · 1019\nRes"}, {"source": "test.pdf", "text": "· 1019\nResidual Dropout\nWe apply dropout [33] to t"}, {"source": "test.pdf", "text": "[33] to the output of each sub-layer, before it i"}, {"source": "test.pdf", "text": "efore it is added to the\nsub-layer input and norma"}, {"source": "test.pdf", "text": "and normalized. In addition, we apply dropout to"}, {"source": "test.pdf", "text": "ropout to the sums of the embeddings and the\nposit"}, {"source": "test.pdf", "text": "the\npositional encodings in both the encoder and"}, {"source": "test.pdf", "text": "coder and decoder stacks. For the base model, we u"}, {"source": "test.pdf", "text": "odel, we use a rate of\nPdrop = 0.1.\nLabel Smoothin"}, {"source": "test.pdf", "text": "l Smoothing\nDuring training, we employed label smo"}, {"source": "test.pdf", "text": "label smoothing of value ϵls = 0.1 [36]. This\nhur"}, {"source": "test.pdf", "text": ". This\nhurts perplexity, as the model learns to be"}, {"source": "test.pdf", "text": "arns to be more unsure, but improves accuracy and"}, {"source": "test.pdf", "text": "uracy and BLEU score.\n6\nResults\n6.1\nMachine Transl"}, {"source": "test.pdf", "text": "ine Translation\nOn the WMT 2014 English-to-German"}, {"source": "test.pdf", "text": "to-German translation task, the big transformer mo"}, {"source": "test.pdf", "text": "sformer model (Transformer (big)\nin Table 2) outpe"}, {"source": "test.pdf", "text": "e 2) outperforms the best previously reported mode"}, {"source": "test.pdf", "text": "orted models (including ensembles) by more than 2."}, {"source": "test.pdf", "text": "re than 2.0\nBLEU, establishing a new state-of-the-"}, {"source": "test.pdf", "text": "te-of-the-art BLEU score of 28.4. The configuratio"}, {"source": "test.pdf", "text": "nfiguration of this model is\nlisted in the bottom"}, {"source": "test.pdf", "text": "he bottom line of Table 3. Training took 3.5 days"}, {"source": "test.pdf", "text": "3.5 days on 8 P100 GPUs. Even our base model\nsurp"}, {"source": "test.pdf", "text": "model\nsurpasses all previously published models an"}, {"source": "test.pdf", "text": "models and ensembles, at a fraction of the traini"}, {"source": "test.pdf", "text": "the training cost of any of\nthe competitive models"}, {"source": "test.pdf", "text": "ive models.\nOn the WMT 2014 English-to-French tran"}, {"source": "test.pdf", "text": "rench translation task, our big model achieves a B"}, {"source": "test.pdf", "text": "hieves a BLEU score of 41.0,\noutperforming all of"}, {"source": "test.pdf", "text": "ng all of the previously published single models,"}, {"source": "test.pdf", "text": "e models, at less than 1/4 the training cost of th"}, {"source": "test.pdf", "text": "cost of the\nprevious state-of-the-art model. The T"}, {"source": "test.pdf", "text": "del. The Transformer (big) model trained for Engli"}, {"source": "test.pdf", "text": "for English-to-French used\ndropout rate Pdrop = 0"}, {"source": "test.pdf", "text": "Pdrop = 0.1, instead of 0.3.\nFor the base models,"}, {"source": "test.pdf", "text": "se models, we used a single model obtained by aver"}, {"source": "test.pdf", "text": "ed by averaging the last 5 checkpoints, which\nwere"}, {"source": "test.pdf", "text": "which\nwere written at 10-minute intervals. For the"}, {"source": "test.pdf", "text": "s. For the big models, we averaged the last 20 che"}, {"source": "test.pdf", "text": "ast 20 checkpoints. We\nused beam search with a bea"}, {"source": "test.pdf", "text": "with a beam size of 4 and length penalty α = 0.6 ["}, {"source": "test.pdf", "text": "α = 0.6 [38]. These hyperparameters\nwere chosen a"}, {"source": "test.pdf", "text": "e chosen after experimentation on the development"}, {"source": "test.pdf", "text": "velopment set. We set the maximum output length du"}, {"source": "test.pdf", "text": "length during\ninference to input length + 50, but"}, {"source": "test.pdf", "text": "+ 50, but terminate early when possible [38].\nTab"}, {"source": "test.pdf", "text": "[38].\nTable 2 summarizes our results and compares"}, {"source": "test.pdf", "text": "d compares our translation quality and training co"}, {"source": "test.pdf", "text": "raining costs to other model\narchitectures from th"}, {"source": "test.pdf", "text": "es from the literature. We estimate the number of"}, {"source": "test.pdf", "text": "number of floating point operations used to train"}, {"source": "test.pdf", "text": "to train a\nmodel by multiplying the training time"}, {"source": "test.pdf", "text": "ining time, the number of GPUs used, and an estima"}, {"source": "test.pdf", "text": "an estimate of the sustained\nsingle-precision flo"}, {"source": "test.pdf", "text": "cision floating-point capacity of each GPU 5.\n6.2"}, {"source": "test.pdf", "text": "PU 5.\n6.2\nModel Variations\nTo evaluate the importa"}, {"source": "test.pdf", "text": "he importance of different components of the Trans"}, {"source": "test.pdf", "text": "the Transformer, we varied our base model\nin diff"}, {"source": "test.pdf", "text": "el\nin different ways, measuring the change in perf"}, {"source": "test.pdf", "text": "ge in performance on English-to-German translation"}, {"source": "test.pdf", "text": "ranslation on the\n5We used values of 2.8, 3.7, 6.0"}, {"source": "test.pdf", "text": ", 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P1"}, {"source": "test.pdf", "text": "M40 and P100, respectively.\n8\nTable 3: Variations"}, {"source": "test.pdf", "text": "ariations on the Transformer architecture. Unliste"}, {"source": "test.pdf", "text": "e. Unlisted values are identical to those of the b"}, {"source": "test.pdf", "text": "e of the base\nmodel. All metrics are on the Englis"}, {"source": "test.pdf", "text": "the English-to-German translation development set,"}, {"source": "test.pdf", "text": "pment set, newstest2013. Listed\nperplexities are p"}, {"source": "test.pdf", "text": "ties are per-wordpiece, according to our byte-pair"}, {"source": "test.pdf", "text": "byte-pair encoding, and should not be compared to"}, {"source": "test.pdf", "text": "ompared to\nper-word perplexities.\nN\ndmodel\ndff\nh\nd"}, {"source": "test.pdf", "text": "el\ndff\nh\ndk\ndv\nPdrop\nϵls\ntrain\nPPL\nBLEU\nparams\nste"}, {"source": "test.pdf", "text": "params\nsteps\n(dev)\n(dev)\n×106\nbase\n6\n512\n2048\n8\n64"}, {"source": "test.pdf", "text": "2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512"}, {"source": "test.pdf", "text": "(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32"}, {"source": "test.pdf", "text": "5.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5"}, {"source": "test.pdf", "text": "4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n2"}, {"source": "test.pdf", "text": ")\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256"}, {"source": "test.pdf", "text": "5.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26"}, {"source": "test.pdf", "text": "28\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2"}, {"source": "test.pdf", "text": "4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4"}, {"source": "test.pdf", "text": "25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional em"}, {"source": "test.pdf", "text": "itional embedding instead of sinusoids\n4.92\n25.7\nb"}, {"source": "test.pdf", "text": ".92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213"}, {"source": "test.pdf", "text": "3\n26.4\n213\ndevelopment set, newstest2013. We used"}, {"source": "test.pdf", "text": ". We used beam search as described in the previous"}, {"source": "test.pdf", "text": "e previous section, but no\ncheckpoint averaging. W"}, {"source": "test.pdf", "text": "eraging. We present these results in Table 3.\nIn T"}, {"source": "test.pdf", "text": "le 3.\nIn Table 3 rows (A), we vary the number of a"}, {"source": "test.pdf", "text": "umber of attention heads and the attention key and"}, {"source": "test.pdf", "text": "on key and value dimensions,\nkeeping the amount of"}, {"source": "test.pdf", "text": "amount of computation constant, as described in S"}, {"source": "test.pdf", "text": "ribed in Section 3.2.2. While single-head\nattentio"}, {"source": "test.pdf", "text": "d\nattention is 0.9 BLEU worse than the best settin"}, {"source": "test.pdf", "text": "est setting, quality also drops off with too many"}, {"source": "test.pdf", "text": "too many heads.\nIn Table 3 rows (B), we observe t"}, {"source": "test.pdf", "text": "observe that reducing the attention key size dk h"}, {"source": "test.pdf", "text": "size dk hurts model quality. This\nsuggests that d"}, {"source": "test.pdf", "text": "sts that determining compatibility is not easy and"}, {"source": "test.pdf", "text": "t easy and that a more sophisticated compatibility"}, {"source": "test.pdf", "text": "patibility\nfunction than dot product may be benefi"}, {"source": "test.pdf", "text": "be beneficial. We further observe in rows (C) and"}, {"source": "test.pdf", "text": "ws (C) and (D) that, as expected,\nbigger models ar"}, {"source": "test.pdf", "text": "models are better, and dropout is very helpful in"}, {"source": "test.pdf", "text": "helpful in avoiding over-fitting. In row (E) we re"}, {"source": "test.pdf", "text": "(E) we replace our\nsinusoidal positional encoding"}, {"source": "test.pdf", "text": "l encoding with learned positional embeddings [9],"}, {"source": "test.pdf", "text": "dings [9], and observe nearly identical\nresults to"}, {"source": "test.pdf", "text": "results to the base model.\n6.3\nEnglish Constituenc"}, {"source": "test.pdf", "text": "onstituency Parsing\nTo evaluate if the Transformer"}, {"source": "test.pdf", "text": "ransformer can generalize to other tasks we perfor"}, {"source": "test.pdf", "text": "we performed experiments on English\nconstituency"}, {"source": "test.pdf", "text": "stituency parsing. This task presents specific cha"}, {"source": "test.pdf", "text": "ecific challenges: the output is subject to strong"}, {"source": "test.pdf", "text": "to strong structural\nconstraints and is significa"}, {"source": "test.pdf", "text": "significantly longer than the input. Furthermore,"}, {"source": "test.pdf", "text": "rthermore, RNN sequence-to-sequence\nmodels have no"}, {"source": "test.pdf", "text": "ls have not been able to attain state-of-the-art r"}, {"source": "test.pdf", "text": "-the-art results in small-data regimes [37].\nWe tr"}, {"source": "test.pdf", "text": "37].\nWe trained a 4-layer transformer with dmodel"}, {"source": "test.pdf", "text": "th dmodel = 1024 on the Wall Street Journal (WSJ)"}, {"source": "test.pdf", "text": "nal (WSJ) portion of the\nPenn Treebank [25], about"}, {"source": "test.pdf", "text": "25], about 40K training sentences. We also trained"}, {"source": "test.pdf", "text": "so trained it in a semi-supervised setting,\nusing"}, {"source": "test.pdf", "text": "ng,\nusing the larger high-confidence and BerkleyPa"}, {"source": "test.pdf", "text": "BerkleyParser corpora from with approximately 17M"}, {"source": "test.pdf", "text": "mately 17M sentences\n[37]. We used a vocabulary of"}, {"source": "test.pdf", "text": "abulary of 16K tokens for the WSJ only setting and"}, {"source": "test.pdf", "text": "etting and a vocabulary of 32K tokens\nfor the semi"}, {"source": "test.pdf", "text": "r the semi-supervised setting.\nWe performed only a"}, {"source": "test.pdf", "text": "med only a small number of experiments to select t"}, {"source": "test.pdf", "text": "o select the dropout, both attention and residual"}, {"source": "test.pdf", "text": "residual\n(section 5.4), learning rates and beam s"}, {"source": "test.pdf", "text": "and beam size on the Section 22 development set, a"}, {"source": "test.pdf", "text": "ent set, all other parameters\nremained unchanged f"}, {"source": "test.pdf", "text": "nchanged from the English-to-German base translati"}, {"source": "test.pdf", "text": "translation model. During inference, we\n9\nTable 4"}, {"source": "test.pdf", "text": "9\nTable 4: The Transformer generalizes well to En"}, {"source": "test.pdf", "text": "well to English constituency parsing (Results are"}, {"source": "test.pdf", "text": "sults are on Section 23\nof WSJ)\nParser\nTraining\nWS"}, {"source": "test.pdf", "text": "raining\nWSJ 23 F1\nVinyals & Kaiser el al. (2014) ["}, {"source": "test.pdf", "text": ". (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov"}, {"source": "test.pdf", "text": "8.3\nPetrov et al. (2006) [29]\nWSJ only, discrimina"}, {"source": "test.pdf", "text": "discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ onl"}, {"source": "test.pdf", "text": "0]\nWSJ only, discriminative\n90.4\nDyer et al. (2016"}, {"source": "test.pdf", "text": "al. (2016) [8]\nWSJ only, discriminative\n91.7\nTran"}, {"source": "test.pdf", "text": "91.7\nTransformer (4 layers)\nWSJ only, discriminat"}, {"source": "test.pdf", "text": "iscriminative\n91.3\nZhu et al. (2013) [40]\nsemi-sup"}, {"source": "test.pdf", "text": "]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]"}, {"source": "test.pdf", "text": "009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (20"}, {"source": "test.pdf", "text": "et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals &"}, {"source": "test.pdf", "text": "Vinyals & Kaiser el al. (2014) [37]\nsemi-supervise"}, {"source": "test.pdf", "text": "-supervised\n92.1\nTransformer (4 layers)\nsemi-super"}, {"source": "test.pdf", "text": "semi-supervised\n92.7\nLuong et al. (2015) [23]\nmult"}, {"source": "test.pdf", "text": "[23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngener"}, {"source": "test.pdf", "text": "[8]\ngenerative\n93.3\nincreased the maximum output"}, {"source": "test.pdf", "text": "um output length to input length + 300. We used a"}, {"source": "test.pdf", "text": "We used a beam size of 21 and α = 0.3\nfor both WSJ"}, {"source": "test.pdf", "text": "r both WSJ only and the semi-supervised setting.\nO"}, {"source": "test.pdf", "text": "setting.\nOur results in Table 4 show that despite"}, {"source": "test.pdf", "text": "t despite the lack of task-specific tuning our mod"}, {"source": "test.pdf", "text": "ng our model performs sur-\nprisingly well, yieldin"}, {"source": "test.pdf", "text": "l, yielding better results than all previously rep"}, {"source": "test.pdf", "text": "iously reported models with the exception of the\nR"}, {"source": "test.pdf", "text": "n of the\nRecurrent Neural Network Grammar [8].\nIn"}, {"source": "test.pdf", "text": "r [8].\nIn contrast to RNN sequence-to-sequence mod"}, {"source": "test.pdf", "text": "quence models [37], the Transformer outperforms th"}, {"source": "test.pdf", "text": "erforms the Berkeley-\nParser [29] even when traini"}, {"source": "test.pdf", "text": "hen training only on the WSJ training set of 40K s"}, {"source": "test.pdf", "text": "t of 40K sentences.\n7\nConclusion\nIn this work, we"}, {"source": "test.pdf", "text": "work, we presented the Transformer, the first seq"}, {"source": "test.pdf", "text": "first sequence transduction model based entirely"}, {"source": "test.pdf", "text": "entirely on\nattention, replacing the recurrent la"}, {"source": "test.pdf", "text": "current layers most commonly used in encoder-decod"}, {"source": "test.pdf", "text": "oder-decoder architectures with\nmulti-headed self-"}, {"source": "test.pdf", "text": "aded self-attention.\nFor translation tasks, the Tr"}, {"source": "test.pdf", "text": "ks, the Transformer can be trained significantly f"}, {"source": "test.pdf", "text": "ficantly faster than architectures based\non recurr"}, {"source": "test.pdf", "text": "on recurrent or convolutional layers. On both WMT"}, {"source": "test.pdf", "text": "n both WMT 2014 English-to-German and WMT 2014\nEng"}, {"source": "test.pdf", "text": "T 2014\nEnglish-to-French translation tasks, we ach"}, {"source": "test.pdf", "text": "ks, we achieve a new state of the art. In the form"}, {"source": "test.pdf", "text": "n the former task our best\nmodel outperforms even"}, {"source": "test.pdf", "text": "orms even all previously reported ensembles.\nWe ar"}, {"source": "test.pdf", "text": "les.\nWe are excited about the future of attention-"}, {"source": "test.pdf", "text": "attention-based models and plan to apply them to o"}, {"source": "test.pdf", "text": "them to other tasks. We\nplan to extend the Transf"}, {"source": "test.pdf", "text": "the Transformer to problems involving input and ou"}, {"source": "test.pdf", "text": "put and output modalities other than text and\nto i"}, {"source": "test.pdf", "text": "t and\nto investigate local, restricted attention m"}, {"source": "test.pdf", "text": "ttention mechanisms to efficiently handle large in"}, {"source": "test.pdf", "text": "e large inputs and outputs\nsuch as images, audio a"}, {"source": "test.pdf", "text": "s, audio and video. Making generation less sequent"}, {"source": "test.pdf", "text": "ss sequential is another research goals of ours.\nT"}, {"source": "test.pdf", "text": "of ours.\nThe code we used to train and evaluate ou"}, {"source": "test.pdf", "text": "valuate our models is available at https://github."}, {"source": "test.pdf", "text": "://github.com/\ntensorflow/tensor2tensor.\nAcknowled"}, {"source": "test.pdf", "text": "Acknowledgements\nWe are grateful to Nal Kalchbren"}, {"source": "test.pdf", "text": "Kalchbrenner and Stephan Gouws for their fruitful"}, {"source": "test.pdf", "text": "r fruitful\ncomments, corrections and inspiration."}, {"source": "test.pdf", "text": "piration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan"}, {"source": "test.pdf", "text": "amie Ryan Kiros, and Geoffrey E Hinton. Layer norm"}, {"source": "test.pdf", "text": "Layer normalization. arXiv preprint\narXiv:1607.064"}, {"source": "test.pdf", "text": "v:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyu"}, {"source": "test.pdf", "text": ", Kyunghyun Cho, and Yoshua Bengio. Neural machine"}, {"source": "test.pdf", "text": "al machine translation by jointly\nlearning to alig"}, {"source": "test.pdf", "text": "ng to align and translate. CoRR, abs/1409.0473, 20"}, {"source": "test.pdf", "text": "9.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-T"}, {"source": "test.pdf", "text": "ie, Minh-Thang Luong, and Quoc V. Le. Massive expl"}, {"source": "test.pdf", "text": "ssive exploration of neural\nmachine translation ar"}, {"source": "test.pdf", "text": "slation architectures. CoRR, abs/1703.03906, 2017."}, {"source": "test.pdf", "text": "906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirell"}, {"source": "test.pdf", "text": "and Mirella Lapata. Long short-term memory-network"}, {"source": "test.pdf", "text": "ry-networks for machine\nreading. arXiv preprint ar"}, {"source": "test.pdf", "text": "reprint arXiv:1601.06733, 2016.\n10\n[5] Kyunghyun C"}, {"source": "test.pdf", "text": "yunghyun Cho, Bart van Merrienboer, Caglar Gulcehr"}, {"source": "test.pdf", "text": "ar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Y"}, {"source": "test.pdf", "text": "enk,\nand Yoshua Bengio. Learning phrase representa"}, {"source": "test.pdf", "text": "representations using rnn encoder-decoder for stat"}, {"source": "test.pdf", "text": "r for statistical\nmachine translation. CoRR, abs/1"}, {"source": "test.pdf", "text": "oRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xc"}, {"source": "test.pdf", "text": "hollet. Xception: Deep learning with depthwise sep"}, {"source": "test.pdf", "text": "thwise separable convolutions. arXiv\npreprint arXi"}, {"source": "test.pdf", "text": "print arXiv:1610.02357, 2016.\n[7] Junyoung Chung,"}, {"source": "test.pdf", "text": "ng Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yosh"}, {"source": "test.pdf", "text": ", and Yoshua Bengio. Empirical evaluation\nof gated"}, {"source": "test.pdf", "text": "n\nof gated recurrent neural networks on sequence m"}, {"source": "test.pdf", "text": "sequence modeling. CoRR, abs/1412.3555, 2014.\n[8]"}, {"source": "test.pdf", "text": "2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Bal"}, {"source": "test.pdf", "text": "Miguel Ballesteros, and Noah A. Smith. Recurrent n"}, {"source": "test.pdf", "text": "ecurrent neural\nnetwork grammars. In Proc. of NAAC"}, {"source": "test.pdf", "text": "c. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli"}, {"source": "test.pdf", "text": "chael Auli, David Grangier, Denis Yarats, and Yann"}, {"source": "test.pdf", "text": ", and Yann N. Dauphin. Convolu-\ntional sequence to"}, {"source": "test.pdf", "text": "equence to sequence learning. arXiv preprint arXiv"}, {"source": "test.pdf", "text": "rint arXiv:1705.03122v2, 2017.\n[10] Alex Graves.\nG"}, {"source": "test.pdf", "text": "Graves.\nGenerating sequences with recurrent neura"}, {"source": "test.pdf", "text": "rent neural networks.\narXiv preprint\narXiv:1308.08"}, {"source": "test.pdf", "text": "iv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang"}, {"source": "test.pdf", "text": "ngyu Zhang, Shaoqing Ren, and Jian Sun. Deep resid"}, {"source": "test.pdf", "text": "Deep residual learning for im-\nage recognition. In"}, {"source": "test.pdf", "text": "nition. In Proceedings of the IEEE Conference on C"}, {"source": "test.pdf", "text": "rence on Computer Vision and Pattern\nRecognition,"}, {"source": "test.pdf", "text": "ognition, pages 770–778, 2016.\n[12] Sepp Hochreite"}, {"source": "test.pdf", "text": "Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jü"}, {"source": "test.pdf", "text": "ni, and Jürgen Schmidhuber. Gradient flow in\nrecur"}, {"source": "test.pdf", "text": "w in\nrecurrent nets: the difficulty of learning lo"}, {"source": "test.pdf", "text": "earning long-term dependencies, 2001.\n[13] Sepp Ho"}, {"source": "test.pdf", "text": "3] Sepp Hochreiter and Jürgen Schmidhuber. Long sh"}, {"source": "test.pdf", "text": "r. Long short-term memory. Neural computation,\n9(8"}, {"source": "test.pdf", "text": "ation,\n9(8):1735–1780, 1997.\n[14] Zhongqiang Huang"}, {"source": "test.pdf", "text": "iang Huang and Mary Harper. Self-training PCFG gra"}, {"source": "test.pdf", "text": "g PCFG grammars with latent annotations\nacross lan"}, {"source": "test.pdf", "text": "across languages. In Proceedings of the 2009 Confe"}, {"source": "test.pdf", "text": "2009 Conference on Empirical Methods in Natural\nLa"}, {"source": "test.pdf", "text": "Natural\nLanguage Processing, pages 832–841. ACL, A"}, {"source": "test.pdf", "text": "41. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol"}, {"source": "test.pdf", "text": "icz, Oriol Vinyals, Mike Schuster, Noam Shazeer, a"}, {"source": "test.pdf", "text": "Shazeer, and Yonghui Wu. Exploring\nthe limits of l"}, {"source": "test.pdf", "text": "imits of language modeling. arXiv preprint arXiv:1"}, {"source": "test.pdf", "text": "nt arXiv:1602.02410, 2016.\n[16] Łukasz Kaiser and"}, {"source": "test.pdf", "text": "aiser and Samy Bengio. Can active memory replace a"}, {"source": "test.pdf", "text": "replace attention? In Advances in Neural\nInformat"}, {"source": "test.pdf", "text": "l\nInformation Processing Systems, (NIPS), 2016.\n[1"}, {"source": "test.pdf", "text": ", 2016.\n[17] Łukasz Kaiser and Ilya Sutskever. Neu"}, {"source": "test.pdf", "text": "kever. Neural GPUs learn algorithms. In Internatio"}, {"source": "test.pdf", "text": "International Conference\non Learning Representatio"}, {"source": "test.pdf", "text": "resentations (ICLR), 2016.\n[18] Nal Kalchbrenner,"}, {"source": "test.pdf", "text": "hbrenner, Lasse Espeholt, Karen Simonyan, Aaron va"}, {"source": "test.pdf", "text": ", Aaron van den Oord, Alex Graves, and Ko-\nray Kav"}, {"source": "test.pdf", "text": "o-\nray Kavukcuoglu. Neural machine translation in"}, {"source": "test.pdf", "text": "lation in linear time. arXiv preprint arXiv:1610.1"}, {"source": "test.pdf", "text": "Xiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton"}, {"source": "test.pdf", "text": "arl Denton, Luong Hoang, and Alexander M. Rush. St"}, {"source": "test.pdf", "text": ". Rush. Structured attention networks.\nIn Internat"}, {"source": "test.pdf", "text": "n International Conference on Learning Representat"}, {"source": "test.pdf", "text": "epresentations, 2017.\n[20] Diederik Kingma and Jim"}, {"source": "test.pdf", "text": "ma and Jimmy Ba. Adam: A method for stochastic opt"}, {"source": "test.pdf", "text": "hastic optimization. In ICLR, 2015.\n[21] Oleksii K"}, {"source": "test.pdf", "text": "Oleksii Kuchaiev and Boris Ginsburg. Factorizatio"}, {"source": "test.pdf", "text": "ctorization tricks for LSTM networks. arXiv prepri"}, {"source": "test.pdf", "text": "Xiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan"}, {"source": "test.pdf", "text": "] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Sa"}, {"source": "test.pdf", "text": "ira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and"}, {"source": "test.pdf", "text": "Zhou, and Yoshua Bengio. A structured self-attent"}, {"source": "test.pdf", "text": "elf-attentive sentence embedding. arXiv preprint\na"}, {"source": "test.pdf", "text": "preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang L"}, {"source": "test.pdf", "text": "nh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol"}, {"source": "test.pdf", "text": "er, Oriol Vinyals, and Lukasz Kaiser. Multi-task\ns"}, {"source": "test.pdf", "text": "lti-task\nsequence to sequence learning. arXiv prep"}, {"source": "test.pdf", "text": "arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-T"}, {"source": "test.pdf", "text": "24] Minh-Thang Luong, Hieu Pham, and Christopher D"}, {"source": "test.pdf", "text": "istopher D Manning. Effective approaches to attent"}, {"source": "test.pdf", "text": "to attention-\nbased neural machine translation. a"}, {"source": "test.pdf", "text": "slation. arXiv preprint arXiv:1508.04025, 2015.\n11"}, {"source": "test.pdf", "text": ", 2015.\n11\n[25] Mitchell P Marcus, Mary Ann Marcin"}, {"source": "test.pdf", "text": "Ann Marcinkiewicz, and Beatrice Santorini. Buildin"}, {"source": "test.pdf", "text": "i. Building a large annotated\ncorpus of english: T"}, {"source": "test.pdf", "text": "english: The penn treebank. Computational linguist"}, {"source": "test.pdf", "text": "l linguistics, 19(2):313–330, 1993.\n[26] David McC"}, {"source": "test.pdf", "text": "David McClosky, Eugene Charniak, and Mark Johnson"}, {"source": "test.pdf", "text": "rk Johnson. Effective self-training for parsing. I"}, {"source": "test.pdf", "text": "parsing. In\nProceedings of the Human Language Tech"}, {"source": "test.pdf", "text": "guage Technology Conference of the NAACL, Main Con"}, {"source": "test.pdf", "text": ", Main Conference,\npages 152–159. ACL, June 2006."}, {"source": "test.pdf", "text": "une 2006.\n[27] Ankur Parikh, Oscar Täckström, Dipa"}, {"source": "test.pdf", "text": "tröm, Dipanjan Das, and Jakob Uszkoreit. A decompo"}, {"source": "test.pdf", "text": "A decomposable attention\nmodel. In Empirical Meth"}, {"source": "test.pdf", "text": "rical Methods in Natural Language Processing, 2016"}, {"source": "test.pdf", "text": "sing, 2016.\n[28] Romain Paulus, Caiming Xiong, and"}, {"source": "test.pdf", "text": "Xiong, and Richard Socher. A deep reinforced model"}, {"source": "test.pdf", "text": "rced model for abstractive\nsummarization. arXiv pr"}, {"source": "test.pdf", "text": ". arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav"}, {"source": "test.pdf", "text": "[29] Slav Petrov, Leon Barrett, Romain Thibaux, a"}, {"source": "test.pdf", "text": "Thibaux, and Dan Klein. Learning accurate, compact"}, {"source": "test.pdf", "text": "e, compact,\nand interpretable tree annotation. In"}, {"source": "test.pdf", "text": "ation. In Proceedings of the 21st International Co"}, {"source": "test.pdf", "text": "ational Conference on\nComputational Linguistics an"}, {"source": "test.pdf", "text": "uistics and 44th Annual Meeting of the ACL, pages"}, {"source": "test.pdf", "text": "CL, pages 433–440. ACL, July\n2006.\n[30] Ofir Press"}, {"source": "test.pdf", "text": "Ofir Press and Lior Wolf. Using the output embeddi"}, {"source": "test.pdf", "text": "ut embedding to improve language models. arXiv\npre"}, {"source": "test.pdf", "text": "arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico"}, {"source": "test.pdf", "text": "[31] Rico Sennrich, Barry Haddow, and Alexandra Bi"}, {"source": "test.pdf", "text": "exandra Birch. Neural machine translation of rare"}, {"source": "test.pdf", "text": "n of rare words\nwith subword units. arXiv preprint"}, {"source": "test.pdf", "text": "v preprint arXiv:1508.07909, 2015.\n[32] Noam Shaze"}, {"source": "test.pdf", "text": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz"}, {"source": "test.pdf", "text": "of Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,"}, {"source": "test.pdf", "text": "y Hinton,\nand Jeff Dean. Outrageously large neural"}, {"source": "test.pdf", "text": "rge neural networks: The sparsely-gated mixture-of"}, {"source": "test.pdf", "text": "mixture-of-experts\nlayer. arXiv preprint arXiv:170"}, {"source": "test.pdf", "text": "arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, G"}, {"source": "test.pdf", "text": "vastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya"}, {"source": "test.pdf", "text": "sky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. D"}, {"source": "test.pdf", "text": "di-\nnov. Dropout: a simple way to prevent neural n"}, {"source": "test.pdf", "text": "t neural networks from overfitting. Journal of Mac"}, {"source": "test.pdf", "text": "nal of Machine\nLearning Research, 15(1):1929–1958,"}, {"source": "test.pdf", "text": "1929–1958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur"}, {"source": "test.pdf", "text": "ar, Arthur Szlam, Jason Weston, and Rob Fergus. En"}, {"source": "test.pdf", "text": "Fergus. End-to-end memory\nnetworks. In C. Cortes,"}, {"source": "test.pdf", "text": ". Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,"}, {"source": "test.pdf", "text": "Sugiyama, and R. Garnett, editors,\nAdvances in Neu"}, {"source": "test.pdf", "text": "ces in Neural Information Processing Systems 28, p"}, {"source": "test.pdf", "text": "tems 28, pages 2440–2448. Curran Associates,\nInc.,"}, {"source": "test.pdf", "text": "tes,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyal"}, {"source": "test.pdf", "text": "iol Vinyals, and Quoc VV Le. Sequence to sequence"}, {"source": "test.pdf", "text": "sequence learning with neural\nnetworks. In Advanc"}, {"source": "test.pdf", "text": "In Advances in Neural Information Processing Syst"}, {"source": "test.pdf", "text": "ssing Systems, pages 3104–3112, 2014.\n[36] Christi"}, {"source": "test.pdf", "text": "6] Christian Szegedy, Vincent Vanhoucke, Sergey Io"}, {"source": "test.pdf", "text": "Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna"}, {"source": "test.pdf", "text": "niew Wojna.\nRethinking the inception architecture"}, {"source": "test.pdf", "text": "hitecture for computer vision. CoRR, abs/1512.0056"}, {"source": "test.pdf", "text": "/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Pet"}, {"source": "test.pdf", "text": ", Koo, Petrov, Sutskever, and Hinton. Grammar as a"}, {"source": "test.pdf", "text": "ammar as a foreign language. In\nAdvances in Neural"}, {"source": "test.pdf", "text": "in Neural Information Processing Systems, 2015.\n["}, {"source": "test.pdf", "text": "s, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng C"}, {"source": "test.pdf", "text": "Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfga"}, {"source": "test.pdf", "text": "zi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin"}, {"source": "test.pdf", "text": "n Cao, Qin Gao, Klaus Macherey, et al. Google’s ne"}, {"source": "test.pdf", "text": "oogle’s neural machine\ntranslation system: Bridgin"}, {"source": "test.pdf", "text": "m: Bridging the gap between human and machine tran"}, {"source": "test.pdf", "text": "chine translation. arXiv preprint\narXiv:1609.08144"}, {"source": "test.pdf", "text": "1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang"}, {"source": "test.pdf", "text": "o, Xuguang Wang, Peng Li, and Wei Xu. Deep recurre"}, {"source": "test.pdf", "text": "ep recurrent models with\nfast-forward connections"}, {"source": "test.pdf", "text": "nnections for neural machine translation. CoRR, ab"}, {"source": "test.pdf", "text": ". CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue"}, {"source": "test.pdf", "text": "Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jin"}, {"source": "test.pdf", "text": "g, and Jingbo Zhu. Fast and accurate\nshift-reduce"}, {"source": "test.pdf", "text": "ft-reduce constituent parsing. In Proceedings of t"}, {"source": "test.pdf", "text": "dings of the 51st Annual Meeting of the ACL (Volum"}, {"source": "test.pdf", "text": "ACL (Volume\n1: Long Papers), pages 434–443. ACL, A"}, {"source": "test.pdf", "text": "43. ACL, August 2013.\n12\nAttention Visualizations"}, {"source": "test.pdf", "text": "lizations\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof"}, {"source": "test.pdf", "text": "jority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaw"}, {"source": "test.pdf", "text": "ed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor"}, {"source": "test.pdf", "text": "ration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<p"}, {"source": "test.pdf", "text": ".\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis"}, {"source": "test.pdf", "text": "ad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmeric"}, {"source": "test.pdf", "text": "of\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsinc"}, {"source": "test.pdf", "text": "laws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting"}, {"source": "test.pdf", "text": "or\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<p"}, {"source": "test.pdf", "text": ">\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An"}, {"source": "test.pdf", "text": "gure 3: An example of the attention mechanism foll"}, {"source": "test.pdf", "text": "anism following long-distance dependencies in the"}, {"source": "test.pdf", "text": "es in the\nencoder self-attention in layer 5 of 6."}, {"source": "test.pdf", "text": "r 5 of 6. Many of the attention heads attend to a"}, {"source": "test.pdf", "text": "tend to a distant dependency of\nthe verb ‘making’,"}, {"source": "test.pdf", "text": "‘making’, completing the phrase ‘making...more di"}, {"source": "test.pdf", "text": "...more difficult’. Attentions here shown only for"}, {"source": "test.pdf", "text": "n only for\nthe word ‘making’. Different colors rep"}, {"source": "test.pdf", "text": "colors represent different heads. Best viewed in c"}, {"source": "test.pdf", "text": "iewed in color.\n13\nThe\nLaw\nwill\nnever\nbe\nperfect\n,"}, {"source": "test.pdf", "text": "perfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nth"}, {"source": "test.pdf", "text": "just\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinio"}, {"source": "test.pdf", "text": "my\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\npe"}, {"source": "test.pdf", "text": "ever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\nju"}, {"source": "test.pdf", "text": "ould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy"}, {"source": "test.pdf", "text": "ng\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nneve"}, {"source": "test.pdf", "text": "will\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshoul"}, {"source": "test.pdf", "text": "tion\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing"}, {"source": "test.pdf", "text": "e\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwi"}, {"source": "test.pdf", "text": "The\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplicatio"}, {"source": "test.pdf", "text": "application\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nm"}, {"source": "test.pdf", "text": "t\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFig"}, {"source": "test.pdf", "text": "<pad>\nFigure 4: Two attention heads, also in laye"}, {"source": "test.pdf", "text": "so in layer 5 of 6, apparently involved in anaphor"}, {"source": "test.pdf", "text": "in anaphora resolution. Top:\nFull attentions for h"}, {"source": "test.pdf", "text": "ions for head 5. Bottom: Isolated attentions from"}, {"source": "test.pdf", "text": "ions from just the word ‘its’ for attention heads"}, {"source": "test.pdf", "text": "ion heads 5\nand 6. Note that the attentions are ve"}, {"source": "test.pdf", "text": "ons are very sharp for this word.\n14\nThe\nLaw\nwill"}, {"source": "test.pdf", "text": "Law\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\ns"}, {"source": "test.pdf", "text": "lication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmiss"}, {"source": "test.pdf", "text": "e\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLa"}, {"source": "test.pdf", "text": "ad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplic"}, {"source": "test.pdf", "text": "its\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\na"}, {"source": "test.pdf", "text": "what\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>"}, {"source": "test.pdf", "text": "EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits"}, {"source": "test.pdf", "text": ",\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwh"}, {"source": "test.pdf", "text": "this\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS"}, {"source": "test.pdf", "text": "ion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,"}, {"source": "test.pdf", "text": "perfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthi"}, {"source": "test.pdf", "text": "just\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion"}, {"source": "test.pdf", "text": "my\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the att"}, {"source": "test.pdf", "text": "of the attention heads exhibit behaviour that seem"}, {"source": "test.pdf", "text": "that seems related to the structure of the\nsenten"}, {"source": "test.pdf", "text": "the\nsentence. We give two such examples above, fro"}, {"source": "test.pdf", "text": "above, from two different heads from the encoder s"}, {"source": "test.pdf", "text": "encoder self-attention\nat layer 5 of 6. The heads"}, {"source": "test.pdf", "text": "The heads clearly learned to perform different ta"}, {"source": "test.pdf", "text": "fferent tasks.\n15"}, {"source": "test.png", "text": "编 程 题 它\n3. 魔 法 信 余 网 络\n\n在 一 个 古 老 的 王 国 中 , 为 了 拒"}, {"source": "test.png", "text": "中 , 为 了 拒 御 来 自 暗 影 裂 障 的 侵 恰 , 魔 法 师 们 沿 边 境 线 建"}, {"source": "test.png", "text": "沿 边 境 线 建 立 了 一 排 共 座 哨 兵 塔 、 每 座 哨 兵 增 都 配 备 有 一"}, {"source": "test.png", "text": "都 配 备 有 一 定 数 量 的 “ 以 太 信 标 “, 用\n境 的 魔 法 屏 障\n\n我 们"}, {"source": "test.png", "text": "屏 障\n\n我 们 用 一 个 下 标 从 0 开 始\n\n组 一 来 表 示 这 排 哨 兵 塔 ,"}, {"source": "test.png", "text": "排 哨 兵 塔 , 其 中 丁 门 代 表 第 ; 座 哨 兵 塔 当 前 已 激 活 的 以 太"}, {"source": "test.png", "text": "激 活 的 以 太 信 衍 数 量\n\n每 一 个 位 于 哨 兵 塔 ; 的 信 标 , 都 能"}, {"source": "test.png", "text": "信 标 , 都 能 投 射 出 半 径 为 p 的 保 护 辉 光 , 为 所 有 滔 足 距 离"}, {"source": "test.png", "text": "有 滔 足 距 离 条 件 | 一 | < p 的 哨 兵 塔 贡 献 一 份 屈 障 能 量 ,"}, {"source": "test.png", "text": "屈 障 能 量 , 一 座 哨 兵 塔 7 的\n为 所 有 胺 漆 盔 到 它 的 以 太 信 标"}, {"source": "test.png", "text": "的 以 太 信 标 的 总 数\n\n现 在 , 皇 加 议 会 批 准 了 一 项 紧 总 增 援 计"}, {"source": "test.png", "text": "紧 总 增 援 计 划 , 允 许 你 颜 外 部 署 k 个 新 的 以 太 信 余 。 这 些"}, {"source": "test.png", "text": "信 余 。 这 些 信 标 可 以 被 自 由 地 分 配 到 任 意 一 座 或 多 座 哨 兵"}, {"source": "test.png", "text": "或 多 座 哨 兵 塔 中 ( 即 , 同 一 座 哨 兵 塔 可 以 堤 设\n多 个 信 标 )"}, {"source": "test.png", "text": "多 个 信 标 ) 。\n你 的 任 务 是 , 作 为 王 国 的 首 席 战 略 家 , 设 计"}, {"source": "test.png", "text": "略 家 , 设 计 一 个 最 优 的 信 标 部 署 方 案 , 使 得 所 有 哨 兵 塔 中"}, {"source": "test.png", "text": "有 哨 兵 塔 中 * 最 低 的 炼 ““ 最 大 化 ““。 你 需 要 返 回 这 个 可"}, {"source": "test.png", "text": "返 回 这 个 可 以 达 到 的 、 最 大 化 的 最\nE 胡 障 瑛 度 值\n\n时 间 限 制"}, {"source": "test.png", "text": "值\n\n时 间 限 制 : CIC++ 3 秒 , 其 他 语 言 6 称\n空 间 限 制 : CIC"}, {"source": "test.png", "text": "限 制 : CIC++ 256M, 其 他 浩 言 512M"}]